{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Image captioning - Transformer model</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các tài liệu nghiên cứu thêm:\n",
    " - EfficientNetB2:\n",
    " - Swin Transformer: https://viblo.asia/p/paper-explain-swin-transformer-hierarchical-vision-transformer-using-shifted-windows-L4x5xqxmKBM\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.24.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (2.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 5)) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (15.0.6.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.51.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.31.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.11.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (23.3.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (58.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (4.5.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.19.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 3)) (3.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.38.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.16.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\admin\\onedrive\\máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->-r requirements.txt (line 5)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load the cations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_START = '<start>'\n",
    "SEQ_END = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = os.path.join('.','Flickr8k')\n",
    "IMAGE_PATH = os.path.join(BASE, 'Flickr8k_Dataset')\n",
    "CAPTION_PATH = os.path.join(BASE, 'Flickr8k_text')\n",
    "CAPTION_FULL = os.path.join(CAPTION_PATH, 'Flickr8k.token.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_map = defaultdict(list)\n",
    "with open(CAPTION_FULL) as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        image_id = data[0].split('#')[0]\n",
    "        caption = SEQ_START+ ' ' + data[1].strip()+' '+SEQ_END\n",
    "        if not os.path.exists(os.path.join(IMAGE_PATH, image_id+'.npy')):\n",
    "            continue\n",
    "        captions_map[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2477"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(captions_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_image_paths = []\n",
    "for image_id in captions_map:\n",
    "    all_captions.extend(captions_map[image_id])\n",
    "    all_image_paths.extend([os.path.join(IMAGE_PATH, image_id)]* len(captions_map[image_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> A child in a pink dress is climbing up a set of stairs in an entry way . <end>',\n",
       " '<start> A girl going into a wooden building . <end>',\n",
       " '<start> A little girl climbing into a wooden playhouse . <end>',\n",
       " '<start> A little girl climbing the stairs to her playhouse . <end>',\n",
       " '<start> A little girl in a pink dress going into a wooden cabin . <end>',\n",
       " '<start> A black dog and a spotted dog are fighting <end>',\n",
       " '<start> A black dog and a tri-colored dog playing with each other on the road . <end>',\n",
       " '<start> A black dog and a white dog with brown spots are staring at each other in the street . <end>',\n",
       " '<start> Two dogs of different breeds looking at each other on the road . <end>',\n",
       " '<start> Two dogs on pavement moving toward each other . <end>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1000268201_693b08cb0e.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1000268201_693b08cb0e.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1000268201_693b08cb0e.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1000268201_693b08cb0e.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1000268201_693b08cb0e.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1001773457_577c3a7d70.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1001773457_577c3a7d70.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1001773457_577c3a7d70.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1001773457_577c3a7d70.jpg',\n",
       " '.\\\\Flickr8k\\\\Flickr8k_Dataset\\\\1001773457_577c3a7d70.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_paths[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tokenize captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_dataset = tf.data.Dataset.from_tensor_slices(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(input):\n",
    "    input = tf.strings.lower(input)\n",
    "    return tf.strings.regex_replace(input, r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các tham số cho tokenizer(thuật toán tách từ)\n",
    "MAX_LENGTH = 50\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    standardize = standardize,\n",
    "    output_sequence_length = MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Learn the vocabulary from the caption data.\n",
    "tokenizer.adapt(caption_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\OneDrive\\Máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "caption_vectors = caption_dataset.map(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   3    2   44    6    2   91  157    9   99   49    2  361   14  385\n",
      "    6   31 4553  509    5    4    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  3   2  19 320  61   2 183 118   5   4   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(50,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[   3    2   39   19   99   61    2  183 2513    5    4    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[   3    2   39   19   99    7  385   20   59 2513    5    4    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for cap in caption_vectors.take(4):\n",
    "    print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token = '',\n",
    "    vocabulary = tokenizer.get_vocabulary()\n",
    ")\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token = '',\n",
    "    vocabulary = tokenizer.get_vocabulary(),\n",
    "    invert= True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Image feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (260, 260))  # EfficientNetB2 expects this input shape\n",
    "    img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.EfficientNetB2(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet'\n",
    ")\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_image_paths = sorted(set(all_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [03:03<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "image_dataset = tf.data.Dataset.from_tensor_slices(unique_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "try:\n",
    "    for image, path in tqdm(image_dataset):\n",
    "        # batch_features shape == (16, 8, 8, 1408) (16 is batch size)\n",
    "        batch_features = image_features_extract_model(image)\n",
    "        # after reshaping, batch_features shape == (16, 64, 1408)\n",
    "        batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "        # store the features to a numpy file\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Split the data into training, validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_caption_vectors = defaultdict(list)\n",
    "for image_path, caption in zip(all_image_paths, caption_vectors):\n",
    "    image_to_caption_vectors[image_path].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_keys = list(image_to_caption_vectors.keys())\n",
    "random.shuffle(unique_image_paths)\n",
    "slice_index = int(len(unique_image_paths) * 0.8)\n",
    "train_paths, val_paths = unique_image_paths[:slice_index], unique_image_paths[slice_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9905, 9905, 2480, 2480)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train_paths =[]\n",
    "caption_train = []\n",
    "for path in train_paths:\n",
    "    caption_len = len(image_to_caption_vectors[path])\n",
    "    image_train_paths.extend([path] * caption_len)\n",
    "    caption_train.extend(image_to_caption_vectors[path])\n",
    "\n",
    "image_val_paths = []\n",
    "caption_val = []\n",
    "for path in val_paths:\n",
    "    caption_len = len(image_to_caption_vectors[path])\n",
    "    image_val_paths.extend([path] * caption_len)\n",
    "    caption_val.extend(image_to_caption_vectors[path])\n",
    "\n",
    "len(image_train_paths), len(caption_train), len(image_val_paths), len(caption_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Create tf.data dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(image_path, caption):\n",
    "    img_tensor = np.load(image_path.decode('utf-8') + '.npy')\n",
    "    return img_tensor, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_train_paths, caption_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "                            map_func, [item1, item2], [tf.float32, tf.int64]\n",
    "                      )\n",
    "                      , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Transfromer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 4\n",
    "emb_dim = 512\n",
    "fc_dim = 2048\n",
    "num_heads = 8\n",
    "row_size = 8\n",
    "col_size = 8\n",
    "target_vocab_size = VOCAB_SIZE\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_model import Transformer\n",
    "transformer = Transformer(num_layers=num_layer,\n",
    "                          emb_dim=emb_dim,\n",
    "                          num_heads=num_heads,\n",
    "                          fc_dim=fc_dim,\n",
    "                          row_size=row_size,\n",
    "                          col_size=col_size,\n",
    "                          target_vocab_size=VOCAB_SIZE,\n",
    "                          dropout_rate=dropout_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Custom Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(512.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'T' of int64 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Rsqrt}}; Op<name=Rsqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Rsqrt]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(learning_rate, beta_1\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m, beta_2\u001b[39m=\u001b[39;49m\u001b[39m0.98\u001b[39;49m, epsilon\u001b[39m=\u001b[39;49m\u001b[39m1e-9\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\OneDrive\\Máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py:116\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     87\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     88\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    103\u001b[0m ):\n\u001b[0;32m    104\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    105\u001b[0m         name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    106\u001b[0m         weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    115\u001b[0m     )\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_learning_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_learning_rate(learning_rate)\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_1 \u001b[39m=\u001b[39m beta_1\n\u001b[0;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2 \u001b[39m=\u001b[39m beta_2\n",
      "File \u001b[1;32mc:\\Users\\Admin\\OneDrive\\Máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:368\u001b[0m, in \u001b[0;36m_BaseOptimizer._build_learning_rate\u001b[1;34m(self, learning_rate)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39minit_scope():\n\u001b[0;32m    363\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    364\u001b[0m         learning_rate, learning_rate_schedule\u001b[39m.\u001b[39mLearningRateSchedule\n\u001b[0;32m    365\u001b[0m     ):\n\u001b[0;32m    366\u001b[0m         \u001b[39m# Create a variable to hold the current learning rate.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m         current_learning_rate \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m--> 368\u001b[0m             learning_rate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterations)\n\u001b[0;32m    369\u001b[0m         )\n\u001b[0;32m    370\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_learning_rate \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(\n\u001b[0;32m    371\u001b[0m             current_learning_rate,\n\u001b[0;32m    372\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrent_learning_rate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    373\u001b[0m             dtype\u001b[39m=\u001b[39mcurrent_learning_rate\u001b[39m.\u001b[39mdtype,\n\u001b[0;32m    374\u001b[0m             trainable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m         )\n\u001b[0;32m    376\u001b[0m         \u001b[39mreturn\u001b[39;00m learning_rate\n",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m, in \u001b[0;36mCustomSchedule.__call__\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, step):\n\u001b[1;32m---> 11\u001b[0m     arg1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mrsqrt(step)\n\u001b[0;32m     12\u001b[0m     arg2 \u001b[39m=\u001b[39m step \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarmup_steps \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1.5\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mrsqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model) \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mminimum(arg1, arg2)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\OneDrive\\Máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Admin\\OneDrive\\Máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Value for attr 'T' of int64 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Rsqrt}}; Op<name=Rsqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Rsqrt]"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.Lost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./checkpoints/train\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpoint(transformer\u001b[39m=\u001b[39mtransformer,optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[0;32m      4\u001b[0m \u001b[39m#ckpt = tf.train.Checkpoint(transformer=transformer)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ckpt_manager \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpointManager(ckpt, checkpoint_path, max_to_keep\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './checkpoints/train'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
    "#ckpt = tf.train.Checkpoint(transformer=transformer)\n",
    "\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# If a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask import create_look_ahead_mask\n",
    "@tf.function()\n",
    "def train_step(input_tensor, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
    "    dec_padding_mask = None\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(input_tensor, tar_inp, True, None, look_ahead_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3136\\1942399665.py\", line 15, in train_step  *\n        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    NameError: name 'optimizer' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m train_loss\u001b[39m.\u001b[39mreset_states()\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m (batch, (img_tensor, tar)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m----> 9\u001b[0m     train_step(img_tensor, tar)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Batch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     13\u001b[0m             epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, batch, train_loss\u001b[39m.\u001b[39mresult()))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\OneDrive\\Máy tính\\deeplearning\\gk\\transformer\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileamamz3fi.py:16\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(input_tensor, tar)\u001b[0m\n\u001b[0;32m     14\u001b[0m     loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(loss_function), (ag__\u001b[39m.\u001b[39mld(tar_real), ag__\u001b[39m.\u001b[39mld(predictions)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(loss), ag__\u001b[39m.\u001b[39mld(transformer)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 16\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(optimizer)\u001b[39m.\u001b[39mapply_gradients, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(gradients), ag__\u001b[39m.\u001b[39mld(transformer)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     17\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(train_loss), (ag__\u001b[39m.\u001b[39mld(loss),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mNameError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3136\\1942399665.py\", line 15, in train_step  *\n        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    NameError: name 'optimizer' is not defined\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for (batch, (img_tensor, tar)) in enumerate(dataset):\n",
    "        train_step(img_tensor, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    loss_plot.append(train_loss.result())\n",
    "\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(image_path):\n",
    "    if 'https://' in image_path:\n",
    "        image_extension = image_path[-4:]\n",
    "        image_path = tf.keras.utils.get_file('image' + image_extension, origin=image_path)\n",
    "\n",
    "    result, _ = evaluate(image_path)\n",
    "    print('Prediction Caption:', ' '.join(result))\n",
    "    # opening the image\n",
    "    plt.imshow(plt.imread(image_path))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
